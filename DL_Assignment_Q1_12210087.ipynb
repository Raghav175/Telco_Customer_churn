{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2687705",
   "metadata": {},
   "source": [
    "**1.Implement forward propagation (30 points)**\n",
    "\n",
    "**Use the following values for the initial weights. W1= .10, W2 = .27, W3 =0.11, W4 =0.15 , W5 =0.18,  W6= 0.16**\n",
    "\n",
    "**Use input 1 (i1) = 2, and input 2 (i2) = 4**\n",
    "\n",
    "**Report the output (out) that represents prediction for the inputs i1 = 2, and i2 = 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65082801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "0.69\n",
      "Output (out): 0.56\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial weights given as \n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Inputs\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#Applying Forward Propagation to the given nueral network\n",
    "h1 = sigmoid(i1 * W1 + i2 * W2)\n",
    "h2 = sigmoid(i1 * W3 + i2 * W4)\n",
    "out = sigmoid(h1 * W5 + h2 * W6)\n",
    "\n",
    "print(round(h1,2))\n",
    "print(round(h2,2))\n",
    "print(\"Output (out):\", round(out,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730c6e5",
   "metadata": {},
   "source": [
    "**2.Derivation for Backpropagation (40 points)**\n",
    "\n",
    "**Using Mean Square loss as the loss function and learning rate as a parameter  (ø),\n",
    "show all steps needed for backpropagation. You will need to derive equations for all six connection weights.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd873d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: 0.09996420867135783 0.26992841734271567 0.10996032631829743 0.14992065263659485 0.1795430004359235 0.1595945225951169\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "target = 0.8  # Assuming a target value\n",
    "learning_rate = 0.01  #learning rate\n",
    "\n",
    "# Error calculation\n",
    "error = (target - out) ** 2 / 2\n",
    "\n",
    "# Gradients for output layer\n",
    "delta_out = (target - out) * out * (1 - out) \n",
    "delta_W5 = delta_out * h1\n",
    "delta_W6 = delta_out * h2\n",
    "\n",
    "# Gradients for hidden layer\n",
    "delta_h1 = delta_out * W5 * h1 * (1 - h1)\n",
    "delta_h2 = delta_out * W6 * h2 * (1 - h2)\n",
    "delta_W1 = delta_h1 * i1\n",
    "delta_W2 = delta_h1 * i2\n",
    "delta_W3 = delta_h2 * i1\n",
    "delta_W4 = delta_h2 * i2\n",
    "\n",
    "# Weight updates\n",
    "W1 -= learning_rate * delta_W1\n",
    "W2 -= learning_rate * delta_W2\n",
    "W3 -= learning_rate * delta_W3\n",
    "W4 -= learning_rate * delta_W4\n",
    "W5 -= learning_rate * delta_W5\n",
    "W6 -= learning_rate * delta_W6\n",
    "\n",
    "print(\"Updated weights:\", W1, W2, W3, W4, W5, W6) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f48c9",
   "metadata": {},
   "source": [
    "**3.Assuming the true output to be 1, learning rate = 0.05 and MSE loss,**\n",
    "\n",
    "**calculate the new  weights using gradient descent (20 points). Report the new weights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c752aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1076212764913983\n",
      "0.08420824372906456\n",
      "0.07471460110914277\n",
      "Updated weights: 0.09967024859949505 0.2693404971989901 0.10963447984188 0.14926895968376 0.17578958781354675 0.15626426994454287\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial weights\n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Inputs\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "\n",
    "# Target output\n",
    "target = 1\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# --- Forward Propagation ---\n",
    "h1 = sigmoid(i1 * W1 + i2 * W2)\n",
    "h2 = sigmoid(i1 * W3 + i2 * W4)\n",
    "out = sigmoid(h1 * W5 + h2 * W6)\n",
    "\n",
    "# --- Error Calculation ---\n",
    "error = (target - out) ** 2 / 2\n",
    "\n",
    "# --- Backpropagation ---\n",
    "# Gradients for output layer\n",
    "delta_out = (target - out) * out * (1 - out) \n",
    "print(delta_out)\n",
    "delta_W5 = delta_out * h1\n",
    "print(delta_W5)\n",
    "delta_W6 = delta_out * h2\n",
    "print(delta_W6)\n",
    "\n",
    "# Gradients for hidden layer\n",
    "delta_h1 = delta_out * W5 * h1 * (1 - h1)\n",
    "delta_h2 = delta_out * W6 * h2 * (1 - h2)\n",
    "delta_W1 = delta_h1 * i1\n",
    "delta_W2 = delta_h1 * i2\n",
    "delta_W3 = delta_h2 * i1\n",
    "delta_W4 = delta_h2 * i2\n",
    "\n",
    "# --- Weight Updates ---\n",
    "W1 -= learning_rate * delta_W1\n",
    "W2 -= learning_rate * delta_W2\n",
    "W3 -= learning_rate * delta_W3\n",
    "W4 -= learning_rate * delta_W4\n",
    "W5 -= learning_rate * delta_W5\n",
    "W6 -= learning_rate * delta_W6\n",
    "\n",
    "# --- Report New Weights ---\n",
    "print(\"Updated weights:\", W1, W2, W3, W4, W5, W6) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b3563",
   "metadata": {},
   "source": [
    " **4.Implement another Forward Propagation with “Relu” activation function applied to neurons representing “h1” and “h2”.**\n",
    "\n",
    "**Keep the other parameters same as suggested in step 1. (10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7f6872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with ReLU activation: 0.589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial weights (same as before)\n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Inputs (same as before)\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Forward Propagation with ReLU\n",
    "h1 = relu(i1 * W1 + i2 * W2)\n",
    "h2 = relu(i1 * W3 + i2 * W4)\n",
    "\n",
    "# Output layer (using sigmoid as before)\n",
    "out = 1 / (1 + np.exp(-(h1 * W5 + h2 * W6))) \n",
    "\n",
    "print(\"Output with ReLU activation:\", round(out,3)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86216862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ea975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
